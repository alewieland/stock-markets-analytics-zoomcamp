{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1wruXmPvYWB",
    "outputId": "efe182d1-33ec-4dfc-c2da-f42365316719"
   },
   "source": [
    "# Homework 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (1 point): Dummies on Month and Week-of-Month\n",
    "\n",
    "Find the ABSOLUTE CORRELATION VALUE of the most correlated dummy <month-week_of_month> with the binary outcome variable is_positive_growth_5d_future?\n",
    "\n",
    "You saw in the correlation analysis and modeling that September and October may be important seasonal months. In this task, we'll go futher and try to generate dummies for Month and Week-of-month (starting from 1). For example, the first week of October should be coded similar to this: 'October_w1'. Once you've generated the new set of variables, find the most correlated (in absolute value) one with is_positive_growth_5d_future and round it to 3 digits after the comma.\n",
    "\n",
    "NOTE: new dummies will be used as features in the next tasks, please leave them in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Fin Data Sources\n",
    "import yfinance as yf\n",
    "import pandas_datareader as pdr\n",
    "\n",
    "#Data viz\n",
    "import plotly.graph_objs as go\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "import time\n",
    "from datetime import date\n",
    "\n",
    "# for graphs\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alewieland/Documents/stock-analytics-zoomcamp/hw3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_parquet('stocks_df_combined_2024_05_07.parquet.brotli')\n",
    "df = df_full[df_full.Date>='2000-01-01'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Technical patterns count = 61, examples = ['cdl2crows', 'cdl3blackrows', 'cdl3inside', 'cdl3linestrike', 'cdl3outside']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/55/kttwlc6960q7ybcwzdh9gmtr0000gn/T/ipykernel_25441/1508039375.py:5: RuntimeWarning: divide by zero encountered in log\n",
      "  df['ln_volume'] = df.Volume.apply(lambda x: np.log(x))\n"
     ]
    }
   ],
   "source": [
    "OHLCV = ['Open','High','Low','Close','Adj Close_x','Volume']\n",
    "CATEGORICAL = ['Month', 'Weekday', 'Ticker', 'ticker_type']\n",
    "TO_PREDICT = [g for g in df_full.keys() if (g.find('future')>=0)]\n",
    "TO_DROP = ['Year','Date','index_x', 'index_y', 'index', 'Quarter','Adj Close_y'] + CATEGORICAL + OHLCV\n",
    "df['ln_volume'] = df.Volume.apply(lambda x: np.log(x))\n",
    "\n",
    " # manually defined features\n",
    "GROWTH = [g for g in df_full.keys() if (g.find('growth_')==0)&(g.find('future')<0)]\n",
    "CUSTOM_NUMERICAL = ['SMA10', 'SMA20', 'growing_moving_average', 'high_minus_low_relative','volatility', 'ln_volume']\n",
    "TECHNICAL_INDICATORS = ['adx', 'adxr', 'apo', 'aroon_1','aroon_2', 'aroonosc',\n",
    " 'bop', 'cci', 'cmo','dx', 'macd', 'macdsignal', 'macdhist', 'macd_ext',\n",
    " 'macdsignal_ext', 'macdhist_ext', 'macd_fix', 'macdsignal_fix',\n",
    " 'macdhist_fix', 'mfi', 'minus_di', 'mom', 'plus_di', 'dm', 'ppo',\n",
    " 'roc', 'rocp', 'rocr', 'rocr100', 'rsi', 'slowk', 'slowd', 'fastk',\n",
    " 'fastd', 'fastk_rsi', 'fastd_rsi', 'trix', 'ultosc', 'willr',\n",
    " 'ad', 'adosc', 'obv', 'atr', 'natr', 'ht_dcperiod', 'ht_dcphase',\n",
    " 'ht_phasor_inphase', 'ht_phasor_quadrature', 'ht_sine_sine', 'ht_sine_leadsine',\n",
    " 'ht_trendmod', 'avgprice', 'medprice', 'typprice', 'wclprice']\n",
    "TECHNICAL_PATTERNS = [g for g in df_full.keys() if g.find('cdl')>=0]\n",
    "print(f'Technical patterns count = {len(TECHNICAL_PATTERNS)}, examples = {TECHNICAL_PATTERNS[0:5]}')\n",
    "\n",
    "MACRO = ['gdppot_us_yoy', 'gdppot_us_qoq', 'cpi_core_yoy', 'cpi_core_mom', 'FEDFUNDS',\n",
    " 'DGS1', 'DGS5', 'DGS10']\n",
    "NUMERICAL = GROWTH + TECHNICAL_INDICATORS + TECHNICAL_PATTERNS + CUSTOM_NUMERICAL + MACRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/55/kttwlc6960q7ybcwzdh9gmtr0000gn/T/ipykernel_25441/2159284025.py:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['January' 'January' 'January' ... 'May' 'May' 'May']' has dtype incompatible with datetime64[ns], please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:,'Month'] = df.Month.dt.strftime('%B')\n",
      "/var/folders/55/kttwlc6960q7ybcwzdh9gmtr0000gn/T/ipykernel_25441/2159284025.py:2: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '['0' '1' '2' ... '4' '0' '1']' has dtype incompatible with int32, please explicitly cast to a compatible dtype first.\n",
      "  df.loc[:,'Weekday'] = df.Weekday.astype(str)\n"
     ]
    }
   ],
   "source": [
    "df.loc[:,'Month'] = df.Month.dt.strftime('%B')\n",
    "df.loc[:,'Weekday'] = df.Weekday.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month_wom'] = df.apply(lambda row: f'{row[\"Month\"]}-{(row[\"Date\"].day-1)//7+1}', axis=1)\n",
    "\n",
    "CATEGORICAL.append('month_wom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest value is 0.034536876505795046 for month_wom_September-3\n"
     ]
    }
   ],
   "source": [
    "dummy_variables = pd.get_dummies(df[CATEGORICAL], dtype='int32')\n",
    "\n",
    "DUMMIES = dummy_variables.columns\n",
    "df_with_dummies = pd.concat([df, dummy_variables], axis=1)\n",
    "df_with_corr = df_with_dummies[DUMMIES].corrwith(df['is_positive_growth_5d_future'])\n",
    "df_with_corr.apply(np.abs).sort_values(ascending=False)\n",
    "\n",
    "print(f'The highest value is {df_with_corr.apply(np.abs).sort_values(ascending=False).iloc[0]} for {df_with_corr.apply(np.abs).sort_values(ascending=False).index[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (2 points): Define new \"hand\" rules on macro and technical indicators variables\n",
    "\n",
    "What is the precision score for the best of the NEW predictions (pred3 or pred4), rounded to 3 digits after the comma?\n",
    "\n",
    "Let's utilize the knowledge from the visualised tree (clf10) (Code Snippet 5: 1.4.4 Visualisation):\n",
    "\n",
    "You're asked to define two new 'hand' rules (leading to 'positive' subtrees):\n",
    "\n",
    "pred3_manual_gdp_fastd: (gdppot_us_yoy <= 0.027) & (fastd >= 0.251)\n",
    "pred4_manual_gdp_wti_oil: (gdppot_us_yoy >= 0.027) & (growth_wti_oil_30d <= 1.005)\n",
    "Extend the Code Snippet 3 (Manual \"hand rule\" predictions): Calculate and add new rules (pred3 and pred4) to the dataframe.You should notice that one of the predictions doesn't have any positive predictions on TEST dataset (while it has many on TRAIN+VALIDATION).\n",
    "\n",
    "Debug: check in the new_df and the original dataset/data generation process that we didn't make any mistakes during the data transformation step.\n",
    "\n",
    "Explain why this can happen even if there are no errors in the data features.\n",
    "\n",
    "As a result, write down the precision score for the remaining predictor (round to three decimal points). E.g. if you have 0.57897, your answer should be 0.579."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_split(df, min_date, max_date, train_prop=0.7, val_prop=0.15, test_prop=0.15):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into three buckets based on the temporal order of the 'Date' column.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The DataFrame to split.\n",
    "        min_date (str or Timestamp): Minimum date in the DataFrame.\n",
    "        max_date (str or Timestamp): Maximum date in the DataFrame.\n",
    "        train_prop (float): Proportion of data for training set (default: 0.6).\n",
    "        val_prop (float): Proportion of data for validation set (default: 0.2).\n",
    "        test_prop (float): Proportion of data for test set (default: 0.2).\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The input DataFrame with a new column 'split' indicating the split for each row.\n",
    "    \"\"\"\n",
    "    # Define the date intervals\n",
    "    train_end = min_date + pd.Timedelta(days=(max_date - min_date).days * train_prop)\n",
    "    val_end = train_end + pd.Timedelta(days=(max_date - min_date).days * val_prop)\n",
    "\n",
    "    # Assign split labels based on date ranges\n",
    "    split_labels = []\n",
    "    for date in df['Date']:\n",
    "        if date <= train_end:\n",
    "            split_labels.append('train')\n",
    "        elif date <= val_end:\n",
    "            split_labels.append('validation')\n",
    "        else:\n",
    "            split_labels.append('test')\n",
    "\n",
    "    # Add 'split' column to the DataFrame\n",
    "    df['split'] = split_labels\n",
    "\n",
    "    return df\n",
    "\n",
    "min_date_df = df_with_dummies.Date.min()\n",
    "max_date_df = df_with_dummies.Date.max()\n",
    "\n",
    "df_with_dummies = temporal_split(df_with_dummies,\n",
    "                                 min_date = min_date_df,\n",
    "                                 max_date = max_date_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.555\n"
     ]
    }
   ],
   "source": [
    "# generate manual predictions\n",
    "# Let's label all prediction features with prefix \"pred\"\n",
    "new_df = df_with_dummies.copy()\n",
    "new_df['pred0_manual_cci'] = (new_df.cci>200).astype(int)\n",
    "new_df['pred1_manual_prev_g1'] = (new_df.growth_1d>1).astype(int)\n",
    "new_df['pred2_manual_prev_g1_and_snp'] = ((new_df['growth_1d'] > 1) & (new_df['growth_snp500_1d'] > 1)).astype(int)\n",
    "new_df['pred3_manual_gdp_fastd'] = ((new_df['gdppot_us_yoy'] <= 0.027) & (new_df['fastd'] >= 0.251)).astype(int)\n",
    "new_df['pred4_manual_gdp_wti_oil']=((new_df['gdppot_us_yoy']>=0.027) & (new_df['growth_wti_oil_30d'] <= 1.005)).astype(int)\n",
    "rule_thumb_prediction = new_df[new_df[\"split\"] == 'test'][['pred4_manual_gdp_wti_oil', 'pred3_manual_gdp_fastd', 'is_positive_growth_5d_future']]\n",
    "\n",
    "print(f'Accuracy {round((rule_thumb_prediction[\"pred3_manual_gdp_fastd\"]==rule_thumb_prediction[\"is_positive_growth_5d_future\"]).sum()/len(rule_thumb_prediction),3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (1 point): Unique correct predictions from a 10-levels deep Decision Tree Classifier (pred5_clf_10)\n",
    "\n",
    "* What is the total number of records in the TEST dataset when the new prediction pred5_clf_10 is better than all 'hand' rules (pred0..pred4)?\n",
    "\n",
    "NOTE: please include random_state=42 to Decision Tree Classifier init function (line clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)) to ensure everyone gets the same results.\n",
    "\n",
    "Suggested solution:\n",
    "\n",
    "Step1: Rewrite the '1.4.3 Inference for a decision tree' piece for the Decision Tree Classifier with max_depth=10 (clf_10), so that you fit the model on TRAIN+VALIDATION sets (unchanged from the lecture), but predict on the whole set X_all (to be able to define a new column 'pred5_clf_10' in the dataframe new_df). Here is the link with explanation. It will solve the problem in 1.4.5 when predictions were made only for Test dataset and couldn't be easily joined with the full dataset.\n",
    "\n",
    "Step2: Once you have it, define a new column 'only_pred5_is_correct' similar to 'hand' prediction rules with several conditions: is_positive_growth_5d_future AND is_correct_pred5 should be equal 1, while all other predictions is_correct_pred0..is_correct_pred4 should be equal to 0.\n",
    "\n",
    "Step3: Convert 'only_pred5_is_correct' column from bool to int, and find how many times it is equal to 1 in the TEST set. Write down this as an answer.\n",
    "\n",
    "ADVANCED: define a function that can be applied to the whole row of predictions (a few examples of pandas-apply-row-functions) and can find whether some prediction 'predX' (where X is one of the predictions) is uniquely correct. It should work even if there are 100 predictions available, so that you don't define manually the condition for 'predX'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def remove_infinite_values(X):\n",
    "    \"\"\"\n",
    "    Remove infinite values from the input array.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input array (NumPy array or array-like)\n",
    "\n",
    "    Returns:\n",
    "    - Array with infinite values removed\n",
    "    \"\"\"\n",
    "    return X[np.isfinite(X).all(axis=1)]\n",
    "\n",
    "# Example usage:\n",
    "# Assuming X is your input data\n",
    "# filtered_X = remove_infinite_values(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: X_train (152846, 302),  X_test (29829, 302)\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets based on the split date\n",
    "features_list = NUMERICAL+DUMMIES.to_list()\n",
    "to_predict = 'is_positive_growth_5d_future'\n",
    "\n",
    "train_df = new_df[new_df.split.isin(['train','validation'])].copy(deep=True)\n",
    "test_df = new_df[new_df.split.isin(['test'])].copy(deep=True)\n",
    "\n",
    "# ONLY numerical Separate features and target variable for training and testing sets\n",
    "# need Date and Ticker later when merging predictions to the dataset\n",
    "X_train = train_df[features_list+[to_predict,'Date','Ticker']]\n",
    "X_test = test_df[features_list+[to_predict,'Date','Ticker']]\n",
    "\n",
    "print(f'length: X_train {X_train.shape},  X_test {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: X_train_imputed (152846, 302), X_test_imputed (29829, 302)\n"
     ]
    }
   ],
   "source": [
    "# Disable SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "X_train.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X_test.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Need to fill NaNs somehow\n",
    "X_train.fillna(0, inplace=True)\n",
    "X_test.fillna(0, inplace=True)\n",
    "\n",
    "print(f'length: X_train_imputed {X_train.shape}, X_test_imputed {X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = X_train[to_predict]\n",
    "y_test = X_test[to_predict]\n",
    "\n",
    "# remove y_train, y_test from X_ dataframes\n",
    "del X_train[to_predict]\n",
    "del X_test[to_predict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score\n",
    "def fit_decision_tree(X, y, max_depth=20):\n",
    "# Initialize the Decision Tree Classifier\n",
    "  clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "\n",
    "  # Fit the classifier to the training data\n",
    "  clf.fit(X, y)\n",
    "  return clf, X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_10, train_columns = fit_decision_tree(X=X_train.drop(['Date','Ticker'],axis=1),\n",
    "                           y=y_train,\n",
    "                           max_depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pred0_manual_cci',\n",
       " 'pred1_manual_prev_g1',\n",
       " 'pred2_manual_prev_g1_and_snp',\n",
       " 'pred3_manual_gdp_fastd',\n",
       " 'pred4_manual_gdp_wti_oil']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['pred5_clf10'] = clf_10.predict(new_df[features_list].replace([np.inf, -np.inf], 0))\n",
    "\n",
    "pred_columns = [_pred  for _pred in new_df.columns if 'pred' in _pred]\n",
    "pred_columns.remove( 'pred5_clf10')\n",
    "pred_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def only_tree_is_correct(row):\n",
    "    if (row['pred5_clf10']!=1) and (row['is_positive_growth_5d_future']!=1):\n",
    "        return False\n",
    "    for _pred_col in pred_columns:\n",
    "        if row[_pred_col]==1:\n",
    "            return False\n",
    "    \n",
    "    return True    \n",
    "\n",
    "new_df[new_df.split == 'test'].apply(only_tree_is_correct, axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: (2 points) Hyperparameter tuning for a Decision Tree\n",
    "What is the optimal tree depth (from 1 to 20) for a DecisionTreeClassifier?\n",
    "\n",
    "NOTE: please include random_state=42 to Decision Tree Classifier init function (line clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)) to ensure consistency in results.\n",
    "\n",
    "Follow these steps to find the optimal max_depth:\n",
    "\n",
    "Iterate through max_depth values from 1 to 20.\n",
    "Train the Decision Tree Classifier with the current max_depth parameter.\n",
    "Optionally, visualize how the 'head' of each fitted tree changes with more advanced (=deep) trees. You can use the sklearn.tree.plot_tree() function, or the compact way with the export_text() functionality (Stack Overflow example):\n",
    "```\n",
    "from sklearn.tree import export_text\n",
    "tree_rules = export_text(model, feature_names=list(X_train), max_depth=3)\n",
    "print(tree_rules)\n",
    "```\n",
    "Calculate the precision score (you can use the function sklearn.metrics.precision_score()) on the TEST dataset for each of the fitted trees. You can also compare it with the precision score on a VALIDATION dataset, which is included to the training phase (to have more data to train on). You should see that the precision score on a VALIDATION set starts to grow with the complexity of a tree (overfit), which isn't seen on the precision score on TEST.\n",
    "Identify the optimal max_depth, where the precision score is the highest on the TEST dataset. Record this value as best_max_depth and submit as an answer.\n",
    "Make predictions on all records (TRAIN+VALIDATION+TEST) and add the new prediction pred6_clf_best to the dataframe new_df.\n",
    "Additionally, compare the precision score of the tuned decision tree with previous predictions. You should observe an improvement (>0.58, or more than 58% precision), indicating that the tuned tree outperforms previous manual \"hand\" rules and Decision Tree predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tree with depth 6\n",
      "Accuracy on test: 0.5711735485873952\n",
      "Accuracy on validation: 0.5703184100130665\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "trees_result = defaultdict(int)\n",
    "\n",
    "\n",
    "for k in range(6,7):\n",
    "    print(f'Training tree with depth {k}')\n",
    "    clf, train_columns = fit_decision_tree(X=X_train.drop(['Date','Ticker'],axis=1),\n",
    "                           y=y_train,\n",
    "                           max_depth=k)\n",
    "    print(f'Accuracy on test: {precision_score(y_test, clf.predict(X_test.drop(columns=[\"Date\", \"Ticker\"])))}')\n",
    "    \n",
    "    trees_result[k] = precision_score(y_test, clf.predict(X_test.drop(columns=[\"Date\", \"Ticker\"])))\n",
    "    \n",
    "    y_val = new_df[new_df.split == 'validation'][to_predict]\n",
    "    X_val = new_df[new_df.split == 'validation'][features_list].replace([np.inf, -np.inf], 0)\n",
    "    print(f'Accuracy on validation: {precision_score(y_val, clf.predict(X_val))}')\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training tree with depth 6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['pred0_manual_cci',\n",
       " 'pred1_manual_prev_g1',\n",
       " 'pred2_manual_prev_g1_and_snp',\n",
       " 'pred3_manual_gdp_fastd',\n",
       " 'pred4_manual_gdp_wti_oil',\n",
       " 'pred5_clf10']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retraining with all futures\n",
    "\n",
    "X = new_df.copy(deep=True)\n",
    "\n",
    "X = X[features_list+[to_predict,'Date','Ticker']]\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X.fillna(0, inplace=True)\n",
    "y = X[to_predict]\n",
    "del X[to_predict]\n",
    "\n",
    "print(f'Training tree with depth {6}')\n",
    "clf, train_columns = fit_decision_tree(X=X.drop(['Date','Ticker'],axis=1),\n",
    "                        y=y,\n",
    "                        max_depth=6)\n",
    "\n",
    "\n",
    "new_df['pred6_clf'] = clf.predict(new_df[features_list].replace([np.inf, -np.inf], 0))\n",
    "\n",
    "pred_columns = [_pred  for _pred in new_df.columns if 'pred' in _pred]\n",
    "pred_columns.remove( 'pred6_clf')\n",
    "pred_columns\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|--- gdppot_us_yoy <= 0.03\n",
      "|   |--- fastd <= 0.25\n",
      "|   |   |--- growth_90d <= 0.49\n",
      "|   |   |   |--- growth_dax_30d <= 1.00\n",
      "|   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |--- growth_dax_30d >  1.00\n",
      "|   |   |   |   |--- truncated branch of depth 2\n",
      "|   |   |--- growth_90d >  0.49\n",
      "|   |   |   |--- DGS5 <= 3.62\n",
      "|   |   |   |   |--- class: 0\n",
      "|   |   |   |--- DGS5 >  3.62\n",
      "|   |   |   |   |--- truncated branch of depth 2\n",
      "|   |--- fastd >  0.25\n",
      "|   |   |--- growth_wti_oil_365d <= 1.88\n",
      "|   |   |   |--- growth_gold_90d <= 1.11\n",
      "|   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |--- growth_gold_90d >  1.11\n",
      "|   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |--- growth_wti_oil_365d >  1.88\n",
      "|   |   |   |--- growth_dax_90d <= 0.89\n",
      "|   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |--- growth_dax_90d >  0.89\n",
      "|   |   |   |   |--- truncated branch of depth 3\n",
      "|--- gdppot_us_yoy >  0.03\n",
      "|   |--- growth_wti_oil_30d <= 1.00\n",
      "|   |   |--- growth_dji_90d <= 0.97\n",
      "|   |   |   |--- growth_90d <= 0.93\n",
      "|   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |--- growth_90d >  0.93\n",
      "|   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |--- growth_dji_90d >  0.97\n",
      "|   |   |   |--- month_wom_October-4 <= 0.50\n",
      "|   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |--- month_wom_October-4 >  0.50\n",
      "|   |   |   |   |--- truncated branch of depth 3\n",
      "|   |--- growth_wti_oil_30d >  1.00\n",
      "|   |   |--- Month_September <= 0.50\n",
      "|   |   |   |--- growth_dax_90d <= 0.71\n",
      "|   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |--- growth_dax_90d >  0.71\n",
      "|   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |--- Month_September >  0.50\n",
      "|   |   |   |--- growth_gold_7d <= 1.00\n",
      "|   |   |   |   |--- truncated branch of depth 3\n",
      "|   |   |   |--- growth_gold_7d >  1.00\n",
      "|   |   |   |   |--- truncated branch of depth 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import export_text\n",
    "tree_rules = export_text(clf, feature_names=list(X.drop(columns=['Ticker', 'Date'])), max_depth=3)\n",
    "\n",
    "print(tree_rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
